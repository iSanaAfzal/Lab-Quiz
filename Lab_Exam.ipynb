{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Quiz-2**"
      ],
      "metadata": {
        "id": "wKBOgALplcmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform following tasks on the provided Reviews Dataset.\n",
        "* Drop words if not alphabets.\n",
        "* Tokenize the sentence.\n",
        "* Perform lemitization.\n",
        "* Vectorize using bigram and trigram techniques.\n",
        "* Apply Random Forest algorithm with 150 trees.\n",
        "* Evaluate overall accuracy of the model and class-wise precision ."
      ],
      "metadata": {
        "id": "m5VxQWNuli1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "VsYojZ_32ExJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk scikit-learn\n"
      ],
      "metadata": {
        "id": "bVM9qsDc3C6I",
        "outputId": "cadd1dd5-11f7-47e5-a514-d08f4a4a95af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "tnDzIH3f7zu9",
        "outputId": "6edc4361-6862-46f0-801b-319ab82e027b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('reviews_dataset.csv')\n"
      ],
      "metadata": {
        "id": "pf6gMAVB77HD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dropna(subset=['news', 'type'], inplace=True)\n"
      ],
      "metadata": {
        "id": "lk9ZcN9b8BXw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)\n"
      ],
      "metadata": {
        "id": "-ZYUBOd18JBP",
        "outputId": "1032a597-c246-4a34-c4e5-d295933545ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2225, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "dataset['cleaned_text'] = dataset['news'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "wIAXJ83-8QJ4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['cleaned_text'].head())\n"
      ],
      "metadata": {
        "id": "3VXZBDCa8WUy",
        "outputId": "a48bc640-ce6a-453d-9903-6ad396d53338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    China had role in Yukos splitup\\n \\n China len...\n",
            "1    Oil rebounds from weather effect\\n \\n Oil pric...\n",
            "2    Indonesia declines debt freeze\\n \\n Indonesia ...\n",
            "3    m payoff for former Shell boss\\n \\n Shell is t...\n",
            "4    US bank in m SEC settlement\\n \\n Five Bank of ...\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/usr/nltk_data')\n"
      ],
      "metadata": {
        "id": "neovfdU58z4W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "nltk_data_path = '/root/nltk_data'\n",
        "if os.path.exists(nltk_data_path):\n",
        "    shutil.rmtree(nltk_data_path)\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "6gBN840584Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def tokenize_and_lemmatize(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "\n",
        "dataset['processed_text'] = dataset['cleaned_text'].apply(tokenize_and_lemmatize)\n"
      ],
      "metadata": {
        "id": "YqMGmWrJ-RvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "nzVtHV6zBoui",
        "outputId": "c500951a-2b63-4564-fcb7-36da687ccab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.data.path)\n"
      ],
      "metadata": {
        "id": "35kzWEb1Bp-i",
        "outputId": "0dc5bd49-fde9-4a4e-f6e8-27ba436a8361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/usr/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "dataset = pd.read_csv('reviews_dataset.csv')\n",
        "\n",
        "\n",
        "dataset.dropna(subset=['news', 'type'], inplace=True)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "dataset['cleaned_text'] = dataset['news'].apply(clean_text)\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return tokenizer.tokenize(text.lower())\n",
        "\n",
        "dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text)\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "\n",
        "dataset['processed_text'] = dataset['tokens'].apply(lemmatize_tokens)\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
        "X = vectorizer.fit_transform(dataset['processed_text'])\n",
        "y = dataset['type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClass-wise Precision:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "id": "DvEPj-qKB1Zb",
        "outputId": "9cc7b770-6050-4cc0-e8cc-0153df828bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.86\n",
            "\n",
            "Class-wise Precision:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.82      0.93      0.87       115\n",
            "entertainment       0.92      0.65      0.76        72\n",
            "     politics       0.97      0.87      0.92        76\n",
            "        sport       0.78      1.00      0.88       102\n",
            "         tech       0.97      0.78      0.86        80\n",
            "\n",
            "     accuracy                           0.86       445\n",
            "    macro avg       0.89      0.85      0.86       445\n",
            " weighted avg       0.88      0.86      0.86       445\n",
            "\n"
          ]
        }
      ]
    }
  ]
}